{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (Modified National Institute of Standards and Technology database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files from the following website, these files will be used to train. \n",
    "\n",
    "[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading bytes from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the images from our zipped file in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as pre\n",
    "import gzip\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as kr\n",
    "import sklearn.preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a zipped file and unzip an image. Read in the image as a byte array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the byte array into a 2D array with 28x28 indecies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ~np.array(list(file_content[800:1584])).reshape(28,28).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show that image as a plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x257a9eca6a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a neural network we need some data to train the network and data to test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "# read in labels\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "# Read in images for testing\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "# Read in labels for testing\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mow use keras to create the network.\n",
    "We create a sequential network, add different layers, and compile it using the ADAM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "# using sequential model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu', input_dim=784))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape the arrays produce by the traning images and labels, so that they create 28x28 2D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the images and labels into 28x28 arrays.    \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape using tensor of 600000, with arrays of 28*28= 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the image array \n",
    "inputs = train_img.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an encpde and lit it with the train_lbls, we can identify the type and class of the results for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels in a one-vs-all fashion\n",
    "encoder = pre.LabelBinarizer()\n",
    "\n",
    "# Trains the encoder\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take our inputs with expected outputs to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 14.5481 - acc: 0.0974\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 14.5463 - acc: 0.09750s - loss:\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 14.5463 - acc: 0.0975\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 14.5463 - acc: 0.0975\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 14.5463 - acc: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257a9f27eb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "model.fit(inputs, outputs, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the testing that the neural network is only correct 10% of the time, As we are dealing with digits only between 0-9, this means it is no better off than simply guessing or having a random number generator guess the answer for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our test images and labels to test it.\n",
    "We start by using the test data that we read in earlier, images and labels that were not in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8)\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting another method to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examing the code and understand what each part was doing, Attempting to understand why the accuracy for a nueral network was so low, I changed the way the inputs from the training data was being used. So we run the code the same as before...\n",
    "\n",
    "Solution found: https://datascience.stackexchange.com/questions/38604/too-low-accuracy-on-mnist-dataset-using-a-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "# read in labels\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "# Read in images for testing\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "# Read in labels for testing\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "# Start a neural network, building it by layers.\n",
    "# using sequential model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu', input_dim=784))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# reshape the images and labels into 28x28 arrays.\n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the trainning images and labels reshaped, we then divide the training images by 255 as the values produced range from 0-255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_img/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then change the values for the labels to categorical, this is because the return for the labels is a returned as a vector as such [0,0,1,0,0,0,0,0,0,0] as 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbl = kr.utils.to_categorical(train_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed with the code as previous and the result is displayed very differntly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.8836 - acc: 0.8183\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.3251 - acc: 0.9029\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.2506 - acc: 0.9251\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.1977 - acc: 0.9404\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.1662 - acc: 0.9507\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.1459 - acc: 0.9559\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.1332 - acc: 0.9595\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.1204 - acc: 0.96261s - loss: 0.1183 - acc: 0.96 - ETA: \n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.1114 - acc: 0.9659\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.1028 - acc: 0.9688\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.0984 - acc: 0.9694\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.0916 - acc: 0.9719\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0868 - acc: 0.9730\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.0845 - acc: 0.9734\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0813 - acc: 0.97470s - loss: 0.0814 - acc: 0.9\n"
     ]
    }
   ],
   "source": [
    "# reshape the image array\n",
    "inputs = train_img.reshape(60000, 784)\n",
    "\n",
    "# Binarize labels in a one-vs-all fashion\n",
    "encoder = pre.LabelBinarizer()\n",
    "\n",
    "# Trains the encoder\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "# Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "model.fit(inputs, outputs, epochs=15, batch_size=100)\n",
    "    \n",
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the image that we are about to have the network predict, we are testing with a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x257ae7e25c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = ~np.array(list(test_img[11:12])).reshape(28,28).astype(np.uint8)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by running thepredict classes methods, the response was 6, making the network successful at predicting at least this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_img[11:12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
