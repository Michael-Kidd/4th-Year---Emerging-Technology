{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (Modified National Institute of Standards and Technology database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files from the following website, these files will be used to train. \n",
    "\n",
    "[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading bytes from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the images from our zipped file in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pre\n",
    "import gzip\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as kr\n",
    "import sklearn.preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a zipped file and unzip an image. Read in the image as a byte array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the byte array into a 2D array with 28x28 indecies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ~np.array(list(file_content[800:1584])).reshape(28,28).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show that image as a plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x216802527b8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a neural network we need some data to train the network and data to test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "# read in labels\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "# Read in images for testing\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "# Read in labels for testing\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mow use keras to create the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "# using sequential model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu', input_dim=784))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape the arrays produce by the traning images and labels, so that they create 28x28 2D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the images and labels into 28x28 arrays.    \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape using tensor of 600000, with arrays of 28*28= 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the image array \n",
    "inputs = train_img.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels in a one-vs-all fashion\n",
    "encoder = pre.LabelBinarizer()\n",
    "\n",
    "# Trains the encoder\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 14.4354 - acc: 0.1044\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 14.4351 - acc: 0.1044\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 14.4351 - acc: 0.1044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21680339278>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "model.fit(inputs, outputs, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the testing that the neural network is only correct 10% of the time, As we are dealing with digits only between 0-9, this means it is no better off than simply guessing or having a random number generator guess the answer for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our test images and labels to test it.\n",
    "We start by using the test data that we read in earlier, images and labels that were not in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8)\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
